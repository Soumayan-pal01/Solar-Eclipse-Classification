# -*- coding: utf-8 -*-
"""Solar_Eclipse_Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NOHuZZoZoVmm8D4Wlf6NOd685D_IaX8d
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from datetime import timedelta
import re
from sklearn.metrics import accuracy_score
from datetime import datetime

train_df=pd.read_csv("/content/drive/MyDrive/Solar Eclipse Classification/train.csv")

test_df=pd.read_csv("/content/drive/MyDrive/Solar Eclipse Classification/test.csv")

train_df.head()

test_df.head()

frames = [train_df, test_df]
df = pd.concat(frames, sort=False)

df.info()

df.shape

df.columns

df.columns = df.columns.str.replace(' ','_') # replacing space between words with underscore
df.columns

df.dtypes

df['Eclipse_Type'].value_counts()

def EclipseClean(x):  # only 4 types of lunar eclipse: P, A, T, H 
    if 'P' in x:
        return('P')
    if 'A' in x:
        return('A')
    if 'T' in x:
        return('T')
    if 'H' in x:
        return('H')

df['Eclipse_Type'] = list(map(EclipseClean,df['Eclipse_Type']))
df['Eclipse_Type'].value_counts()

len(df['Eclipse_Type'].value_counts())

df = df.drop(df[df.Eclipse_Type == 'H'].index) #removing hybrid category to avoid unnecessary outliers

df['Eclipse_Type'].value_counts()

df['Latitude_Number'] = df['Latitude'].str.replace('([A-Z]+)', '')
df['Latitude_Letter'] = df['Latitude'].str.extract('([A-Z]+)')

df['Longitude_Number'] = df['Longitude'].str.replace('([A-Z]+)', '')
df['Longitude_Letter'] = df['Longitude'].str.extract('([A-Z]+)')

df.drop(columns =["Latitude", "Longitude"], inplace = True) 
df.head()

df.isnull().sum()

df.dropna(how='any', subset=['Delta_T_(s)','Saros_Number','Gamma','Eclipse_Magnitude','Sun_Altitude'], inplace=True)

df = df.drop(["Path_Width_(km)", "Central_Duration",'Unnamed:_0','Catalog_Number'], axis=1)

df.isnull().sum()

df

def cleaning1(x):     
    if '-' in x:
        x = x.replace('-','')
    return x

df['Cleaned_Dates'] = list(map(cleaning1, df['Calendar_Date']))   #column without -ve symbol
df.head()

def cleaning2(x):                       #extracting months and removing -ve symbols
    if '-' in x:
        x = x.replace('-','')
    return((re.findall('[A-z]+', x))[0])

df['Cleaned_Dates_Month'] = list(map(cleaning2, df['Calendar_Date']))
df['Cleaned_Dates_Month']

def cleaning3(x):                     #extracting years and removing -ve symbols
    if '-' in x:
        x = x.replace('-','')
    temp = re.findall('\d\d\d\d', x)
    if len(temp)>0:
        return temp[0]
    else:
        return temp

df['Cleaned_Dates_Year'] = list(map(cleaning3, df['Calendar_Date']))
df['Cleaned_Dates_Year']

df = df.drop(["Calendar_Date"], axis=1)
df

df_categories = df.select_dtypes(object)
df_categories

#Converting Latitude_number and Longitude_number to float type from object type
df["Latitude_Number"] = pd.to_numeric(df["Latitude_Number"])
df["Longitude_Number"] = pd.to_numeric(df["Longitude_Number"])
df.head()

df.dtypes

df_Categoricals = df.select_dtypes(object)
df_Categoricals.head()

len(df_Categoricals.columns)

len(df.dtypes)

df = df.drop(['Latitude_Number', 'Longitude_Number', 'Latitude_Letter', 'Longitude_Letter', 'Eclipse_Time'], axis=1)

df_Numericals = df._get_numeric_data()
df_Numericals

corr_matrix = df_Numericals.corr()
corr_matrix.head()

fig, ax = plt.subplots(figsize=(8,8))
heatmap = sns.heatmap(corr_matrix, annot =True, ax=ax)
heatmap

# Due to high correlation, we are dropping 'Saros_Number' & 'Lunation_Number'

df = df.drop(['Saros_Number', 'Lunation_Number'], axis=1)

df_Numericals = df._get_numeric_data()
df_Numericals

corr_matrix = df_Numericals.corr()
corr_matrix.head()

fig, ax = plt.subplots(figsize=(8,8))
heatmap = sns.heatmap(corr_matrix, annot =True, ax=ax)
heatmap

plt.hist(df["Eclipse_Type"], bins = len(df["Eclipse_Type"].unique()))
plt.xticks(rotation='vertical')

import pandas as pd
from sklearn import preprocessing

x = df_Numericals.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df2 = pd.DataFrame(x_scaled)

df2.head()

df.head()

df.dtypes

df_Categoricals.head()

df.columns

numericals = df._get_numeric_data()
numericals = pd.DataFrame(numericals)
numericals

from sklearn.preprocessing import Normalizer
transformer = Normalizer().fit(numericals)
normalized_x = transformer.transform(numericals)
pd.DataFrame(normalized_x)

categoricals = df.select_dtypes('object')
categoricals = categoricals['Eclipse_Type']
y = categoricals

import pandas as pd
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt

# defining the target variable (dependent variable) as y

y = df.Eclipse_Type

from sklearn.preprocessing import LabelEncoder 
le = LabelEncoder()  
y = le.fit_transform(y)
y

# creating training and testing variables
# test_size = the percentage of the data for testing. It’s usually around 80/20 or 70/30. In this case 80/20

X_train, X_test, y_train, y_test = train_test_split(normalized_x, y, test_size=0.2)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

"""##Using Linear Regression"""

# fitting the model on the training data


lm = linear_model.LinearRegression()
model = lm.fit(X_train, y_train)
predictions = lm.predict(X_test)

# show first five predicted values
predictions[0:5]

# plotting the model - The line / model

plt.scatter(y_test, predictions)
plt.xlabel(['True_Values'])
plt.ylabel(['Predictions'])

"""##Using Logistic Regression"""

from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression

# Training the Logistic Regression Model:

# Split data into 'X' features and 'y' target label sets

X = normalized_x
y = le.fit_transform(y)

# Import module to split dataset

from sklearn.model_selection import train_test_split

# Split data set into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) 
# random_state= _no._ simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time.

from sklearn.linear_model import LogisticRegression

# Create instance (i.e. object) of LogisticRegression
logmodel = LogisticRegression()

# Fit the model using the training data
# X_train -> parameter supplies the data features
# y_train -> parameter supplies the target labels
logmodel.fit(X_train, y_train)

pd.DataFrame(y_test)

from sklearn.metrics import classification_report, accuracy_score
predictions = logmodel.predict(pd.DataFrame(X_test))

print(classification_report(y_test, predictions))
print(accuracy_score(y_test, predictions))

"""##Using Random Forest Algorithm"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.ensemble import RandomForestClassifier

randomForestClassification = RandomForestClassifier(n_estimators=100,random_state=259)
randomForestClassification.fit(X_train, y_train)
y_pred = randomForestClassification.predict(X_test)

pd.Series(y_pred).value_counts()

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))

randomForestClassification.feature_importances_

"""##SVM ALGORITHM - Support Vector Machine / Classification¶"""

# X = normalized_x
# y = le.fit_transform(y)

from sklearn.svm import SVC # "Support Vector Classifier" 
clf = SVC(kernel='linear') 
  
# fitting x samples and y classes 
clf.fit(X, y)

print(classification_report(y_test, predictions))
print(accuracy_score(y_test, predictions))

"""##Therefore, here we can see that Random Forest Algorithm gives the best Accuracy(84%)"""